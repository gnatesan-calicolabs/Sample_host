:py:mod:`tfrecord_batcher`
==========================

.. py:module:: tfrecord_batcher


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   tfrecord_batcher.TFRecordBatcher



Functions
~~~~~~~~~

.. autoapisummary::

   tfrecord_batcher.tfrecord_dataset
   tfrecord_batcher.tfrecord_dataset_multi
   tfrecord_batcher.make_data_ops
   tfrecord_batcher.num_possible_augmentations
   tfrecord_batcher.make_input_fn
   tfrecord_batcher.order_tfrecords



Attributes
~~~~~~~~~~

.. autoapisummary::

   tfrecord_batcher.SHUFFLE_BUFFER_DEPTH_PER_FILE
   tfrecord_batcher.NUM_FILES_TO_PARALLEL_INTERLEAVE


.. py:data:: SHUFFLE_BUFFER_DEPTH_PER_FILE
   :value: 32

   

.. py:data:: NUM_FILES_TO_PARALLEL_INTERLEAVE
   :value: 8

   

.. py:function:: tfrecord_dataset(tfr_data_files_pattern, batch_size, seq_length, seq_depth, target_length, num_targets, mode, use_static_batch_size=False, repeat=True)

   Load TFRecord format data.

   The tf.Example assumed to be ZLIB compressed with fields:
     sequence: tf.string FixedLenFeature of length seq_length * seq_depth.
     label: tf.float32 FixedLenFeature of target_length * num_targets.

   Args:
    tfr_data_files_pattern: Pattern (potentially with globs) for TFRecord
      format files. See `tf.gfile.Glob` for more information.
     batch_size: batch_size
     seq_length: length of input sequence
     seq_depth: vocabulary size of the inputs (4 for raw DNA)
     target_length: length of the target sequence
     num_targets: number of targets at each target sequence location
     mode: a tf.estimator.ModeKeys instance
     use_static_batch_size: whether to enforce that all batches have a fixed
       batch size. Note that for test data, where we don't take repeated passes,
       setting this to True will drop a few examples from the end of the dataset,
       if batch size doesn't divide the number of test examples, rather than
       causing an exception.
     repeat: repeat the training dataset
   Returns:
     A Dataset which will produce a dict with the following tensors:
       sequence: [batch_size, sequence_length, seq_depth]
       label: [batch_size, num_targets, target_length]
       na: [batch_size, num_targets]


.. py:function:: tfrecord_dataset_multi(tfr_data_files_pattern, batch_size, seq_length, seq_depth, genome_targets, target_length, mode)

   Load TFRecord format data.

     The tf.Example assumed to be ZLIB compressed with fields:
       sequence: tf.string FixedLenFeature of length seq_length * seq_depth.
       label: tf.float32 FixedLenFeature of target_length * num_targets.

     Args:
      tfr_data_file_pattern: Pattern (potentially with globs) for TFRecord
        format files. See `tf.gfile.Glob` for more information.
       batch_size: batch_size
       seq_length: length of input sequence
       seq_depth: vocabulary size of the inputs (4 for raw DNA)
       genome_targets: number of targets for each genome
       target_length: length of the target sequence
   j   mode: a tf.estimator.ModeKeys instance

     Returns:
       A Dataset which will produce a dict with the following tensors:
         sequence: [batch_size, sequence_length, seq_depth]
         label: [batch_size, num_targets, target_length]
         na: [batch_size, num_targets]
     


.. py:function:: make_data_ops(job, files_pattern, mode, use_static_batch_size=False)

   Get an iterator over your training data.

   Args:
     job: a dictionary of parsed parameters.
       See `basenji.google.params.read_job_params` for more information.
     batch_size: the batch size.
     files_pattern: A file path pattern that has your training data. For example,
       '/cns/sandbox/home/mlbileschi/brain/basenji/data/train/*'.
     mode: a tf.estimator.ModeKeys instance.
     use_static_batch_size: whether to enforce that all batches have a fixed
       batch size. Note that for test data, where we don't take repeated passes,
       setting this to True will drop a few examples from the end of the data


.. py:function:: num_possible_augmentations(augment_with_complement, shift_augment_offsets)


.. py:function:: make_input_fn(job, data_file_pattern, mode, use_static_batch_size)

   Makes an input_fn, according to the `Experiment` `input_fn` interface.

   Args:
     job: a dictionary of parsed parameters.
       See `basenji.google.params.read_job_params` for more information.
     data_file_pattern: A file path pattern that has your training data.
       For example, '/cns/sandbox/home/mlbileschi/brain/basenji/data/train/*'.
     mode: a tf.estimator.ModeKeys instance.
     use_static_batch_size: whether to enforce that all batches have a fixed
       batch size. Note that for test data, where we don't take repeated passes,
       setting this to True will drop a few examples from the end of the data
   Returns:
     input_fn to be used by Experiment


.. py:function:: order_tfrecords(tfr_pattern)

   Check for TFRecords files fitting my pattern in succession,
   else return empty list.


.. py:class:: TFRecordBatcher(tfr_data_file_pattern, load_targets, seq_length, seq_depth, target_length, num_targets, mode, NAf=None, batch_size=64, pool_width=1, shuffle=False)

   Bases: :py:obj:`object`

   Load TFRecord format data. Many args are unused and for API-compatibility.

   Args:
     tfr_data_file_pattern: Pattern (potentially with globs) for TFRecord
       format files. See `tf.gfile.Glob` for more information.
     load_targets: whether to load targets (unused)
     seq_length: length of the input sequences
     seq_depth: vocabulary size of the inputs (4 for raw DNA)
     target_length: length of the target sequence
     num_targets: number of targets at each target sequence location
     mode: a tf.estimator.ModeKeys instance
     NAf: (unused)
     batch_size: batch_size
     pool_width: width of pooling layers (unused)
     shuffle: whether the batcher should shuffle the data

   .. py:method:: initialize(sess)


   .. py:method:: next(rc=False, shift=0)


   .. py:method:: reset()



