
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>trainer &#8212; Baskerville 0.01 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-trainer">
<span id="trainer"></span><h1><a class="reference internal" href="#module-trainer" title="trainer"><code class="xref py py-mod docutils literal notranslate"><span class="pre">trainer</span></code></a><a class="headerlink" href="#module-trainer" title="Permalink to this heading">¶</a></h1>
<p>SeqNN trainer</p>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading">¶</a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#trainer.Trainer" title="trainer.Trainer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Trainer</span></code></a></p></td>
<td><p>assigning training settings/input variables to class specific local variables</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#trainer.EarlyStoppingMin" title="trainer.EarlyStoppingMin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EarlyStoppingMin</span></code></a></p></td>
<td><p>Stop training when a monitored quantity has stopped improving.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#trainer.Cyclical1LearningRate" title="trainer.Cyclical1LearningRate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Cyclical1LearningRate</span></code></a></p></td>
<td><p>A LearningRateSchedule that uses cyclical schedule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#trainer.WarmUp" title="trainer.WarmUp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WarmUp</span></code></a></p></td>
<td><p>Applies a warmup schedule on a given learning rate decay schedule.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this heading">¶</a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#trainer.parse_loss" title="trainer.parse_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parse_loss</span></code></a>(loss_label[, strategy, keras_fit, ...])</p></td>
<td><p>Select loss function,(MeanSquaredError, BinaryCrossEntrupy, or Poisson) with various options based on entry.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#trainer.compute_norm" title="trainer.compute_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compute_norm</span></code></a>(x, axis, keepdims)</p></td>
<td><p>Computes euclidean norm</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#trainer.unitwise_norm" title="trainer.unitwise_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unitwise_norm</span></code></a>(x)</p></td>
<td><p>#runs previous compute_norm while parsing different cases.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#trainer.adaptive_clip_grad" title="trainer.adaptive_clip_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adaptive_clip_grad</span></code></a>(parameters, gradients[, ...])</p></td>
<td><p>edits gradient descent based on given parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#trainer.safe_next" title="trainer.safe_next"><code class="xref py py-obj docutils literal notranslate"><span class="pre">safe_next</span></code></a>(data_iter[, retry, sleep])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="trainer.parse_loss">
<span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">parse_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_label</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keras_fit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.parse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Select loss function,(MeanSquaredError, BinaryCrossEntrupy, or Poisson) with various options based on entry.
Possible selectable of loss functions from <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/">https://www.tensorflow.org/api_docs/python/tf/keras/losses/</a></p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>loss_label (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>):</dt><dd><p>string indicating chosen loss function.</p>
</dd>
<dt>strategy (<code class="xref py py-obj docutils literal notranslate"><span class="pre">var</span></code>):</dt><dd><p>Determines whether reduction parameter (<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction">https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction</a>) is applied.</p>
</dd>
<dt>keras_fit (<code class="xref py py-obj docutils literal notranslate"><span class="pre">bool</span></code>):</dt><dd><p>Secondary boolean parameter Determining whether reduction parameter is applied (see strategy above)</p>
</dd>
<dt>spec_weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1):</dt><dd><p>term for functions without reduction parameter applied</p>
</dd>
<dt>total_weight (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>term for Poisson Multinomial function</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>loss_fn (<code class="xref py py-obj docutils literal notranslate"><span class="pre">function</span></code>):</dt><dd><p>Selected loss function for classes</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trainer.Trainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">Trainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_gpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keras_fit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer" title="Permalink to this definition">¶</a></dt>
<dd><p>assigning training settings/input variables to class specific local variables</p>
<dl class="py method">
<dt class="sig sig-object py" id="trainer.Trainer.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqnn_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer.compile" title="Permalink to this definition">¶</a></dt>
<dd><p>This creates a diagnostic  ability curve to the input data. If BinaryCrossEntropy, ROC/PR curve is used
If more than this takes place, a pearsons correlation is run on the regression, This is used to create the
loss function</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>self (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</dt><dd><p>variables and settings associated with self trainer initialization</p>
</dd>
</dl>
<p>seqnn_model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.Trainer.fit_keras">
<span class="sig-name descname"><span class="pre">fit_keras</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqnn_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer.fit_keras" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses genomes and reads to create and fit models, using the keras architecture</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>self (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</dt><dd><p>variables and settings associated with self trainer initialization</p>
</dd>
</dl>
<p>seqnn_model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.Trainer.fit2">
<span class="sig-name descname"><span class="pre">fit2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqnn_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unit_test_bool</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer.fit2" title="Permalink to this definition">¶</a></dt>
<dd><p>Is used to parse mouse and human genomes to create a single model, comparing the fits against each other</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>self (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</dt><dd><p>variables and settings associated with self trainer initialization</p>
</dd>
</dl>
<p>seqnn_model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.Trainer.fit_tape">
<span class="sig-name descname"><span class="pre">fit_tape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqnn_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unit_test_bool</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer.fit_tape" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to fit2 in structure and function, except uses one tape model.
Strategy refers to gpu usage, where none == 1 gpu</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>self (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</dt><dd><p>variables and settings associated with self trainer initialization</p>
</dd>
</dl>
<p>seqnn_model (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.Trainer.make_optimizer">
<span class="sig-name descname"><span class="pre">make_optimizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Trainer.make_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>accepts parameters defining optimization strategy, choosing algorithm, (stochastic grad descent, or others)
Start learning rate small and incriment as time goes on, step size is constant until final steps where it drops.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>self (<code class="xref py py-obj docutils literal notranslate"><span class="pre">class</span></code>):</dt><dd><p>variables and settings associated with self trainer initialization</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trainer.compute_norm">
<span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">compute_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdims</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.compute_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes euclidean norm</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trainer.unitwise_norm">
<span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">unitwise_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.unitwise_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>#runs previous compute_norm while parsing different cases.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trainer.adaptive_clip_grad">
<span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">adaptive_clip_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.adaptive_clip_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>edits gradient descent based on given parameters.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trainer.EarlyStoppingMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">EarlyStoppingMin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.EarlyStoppingMin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.callbacks.EarlyStopping</span></code></p>
<p>Stop training when a monitored quantity has stopped improving.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>min_epoch: Minimum number of epochs before considering stopping.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="trainer.EarlyStoppingMin.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.EarlyStoppingMin.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trainer.Cyclical1LearningRate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">Cyclical1LearningRate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maximal_learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'Cyclical1LearningRate'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Cyclical1LearningRate" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.optimizers.schedules.LearningRateSchedule</span></code></p>
<p>A LearningRateSchedule that uses cyclical schedule.
<a class="reference external" href="https://yashuseth.blog/2018/11/26/hyper-parameter-tuning-best-practices-learning-rate-batch-size-momentum-weight-decay/">https://yashuseth.blog/2018/11/26/hyper-parameter-tuning-best-practices-learning-rate-batch-size-momentum-weight-decay/</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="trainer.Cyclical1LearningRate.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Cyclical1LearningRate.__call__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.Cyclical1LearningRate.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trainer.Cyclical1LearningRate.get_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="trainer.WarmUp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">WarmUp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_schedule</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.WarmUp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.optimizers.schedules.LearningRateSchedule</span></code></p>
<p>Applies a warmup schedule on a given learning rate decay schedule.
(h/t HuggingFace.)
Args:</p>
<blockquote>
<div><dl class="simple">
<dt>initial_learning_rate (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>):</dt><dd><p>The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).</p>
</dd>
<dt>decay_schedule (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>):</dt><dd><p>The learning rate or schedule function to apply after the warmup for the rest of training.</p>
</dd>
<dt>warmup_steps (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>):</dt><dd><p>The number of steps for the warmup part of training.</p>
</dd>
<dt>power (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1):</dt><dd><p>The power to use for the polynomial warmup (defaults is a linear warmup).</p>
</dd>
<dt>name (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>):</dt><dd><p>Optional name prefix for the returned tensors during the schedule.</p>
</dd>
</dl>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="trainer.WarmUp.__call__">
<span class="sig-name descname"><span class="pre">__call__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.WarmUp.__call__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="trainer.WarmUp.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#trainer.WarmUp.get_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="trainer.safe_next">
<span class="sig-prename descclassname"><span class="pre">trainer.</span></span><span class="sig-name descname"><span class="pre">safe_next</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">retry</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sleep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#trainer.safe_next" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Baskerville</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, David Kelley, Gunalan Natesan.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 6.1.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/autoapi/trainer/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>